{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.5.3-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optical Character Recognition\n",
        "\n",
        "A common computer vision challenge is to detect and interpret text in an image. This kind of processing is often referred to as *optical character recognition* (OCR).\n",
        "\n",
        "<p style='text-align:center'><img src='./images/ocr.jpg' alt='A robot reading a newspaper'/></p>\n",
        "\n",
        "## Use the Computer Vision Service to Read Text in an Image\n",
        "\n",
        "Let's start with a simple example. The **Computer Vision** cognitive service has some built-in OCR capabilities that enable you to detect the location of text in an image. Let's put it to the test to see if Northwind Traders can use in-store cameras to read text on produce signs.\n",
        "\n",
        "Click the green <span style=\"color:green\">&#9655</span> button at the top left of the cell below to run it and see an example of a sign that you'll get the Computer Vision service to read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Open /data/sign.jpg and display it.\n",
        "image_path = os.path.join('data','vision', 'sign.jpg')\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Create a Cognitive Services resource\n",
        "\n",
        "If you haven't already done so, create a **Cognitive Services** resource in your Azure subscription.\n",
        "\n",
        "> **Tip**: If you did this in the previous lab, you can skip this.\n",
        "\n",
        "1. In another browser tab, open the Azure portal (https://portal.azure.com), and sign in with your Microsoft account.\n",
        "2. Click the **&#65291;Create a resource** button, search for *Cognitive Services*, and create a **Cognitive Services** resource with the following settings:\n",
        "    - **Name**: *Enter a unique name*.\n",
        "    - **Subscription**: *Your Azure subscription*.\n",
        "    - **Location**: *Any available location*.\n",
        "    - **Pricing tier**: S0\n",
        "    - **Resource group**: *Create a resource group with a unique name*.\n",
        "3. Wait for deployment to complete. Then go to your cognitive services resource, and on the **Quick start** page, note the keys and endpoint. You will need these to connect to your cognitive services resource from client applications.\n",
        "\n",
        "### Create variables for your Cognitive Services key and endpoint\n",
        "\n",
        "You'll need the key and endpoint for your Cognitive Services resource to connect to it.\n",
        "\n",
        "1. In the Azure portal, view the **Quick start** page for your Cognitive Services resource.\n",
        "2. Copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
        "3. Copy the **endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n",
        "4. Run the code in the cell below by clicking its green <span style=\"color:green\">&#9655</span> button."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Now you're ready to use the Computer Vision service to read text in an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import os\n",
        "\n",
        "# Get a client for the computer vision service\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Read the image file\n",
        "image_path = os.path.join('data', 'vision', 'sign.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Use the Computer Vision service to find text in the image\n",
        "ocr_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
        "for region in ocr_results.regions:\n",
        "    for line in region.lines:\n",
        "        for word in line.words:\n",
        "            print(word.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Extract Information from Forms\n",
        "\n",
        "A more advanced OCR scenario is the extraction of information from forms or invoices. The **Form Recognizer** service is specifically designed for this kind of AI problem.\n",
        "\n",
        "In this example, you'll use the Form Recognizer's built-in model for analyzing receipts.\n",
        "\n",
        "Run the cell below to see an example of a receipt that you'll use Form Recognizer to analyze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# Load and display a receipt image\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "image_path = os.path.join('data', 'vision', 'receipt.jpg')\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Create a Form Recognizer Resource\n",
        "\n",
        "Start by creating a Form Recognizer resource in your Azure subscription:\n",
        "\n",
        "1. In another browser tab, open the Azure portal (<a href='https://portal.azure.com' target='_blank'>https://portal.azure.com</a>), signing in with your Microsoft account.\n",
        "2. Select **+ Create a resource**, and search for *Form Recognizer*.\n",
        "3. In the list of services, select **Form Recognizer**.\n",
        "4. In the **Form Recognizer** blade, select **Create**.\n",
        "5. In the **Create** blade, enter the following details and select **Create**\n",
        "  * **Name**: A unique name for your service\n",
        "  * **Subscription**: Your Azure subscription\n",
        "  * **Location**: Any available location\n",
        "  * **Pricing tier**: F0\n",
        "  * **Resource Group**: The existing resource group you used previously\n",
        "  * **I confirm I have read and understood the notice below**: Selected.\n",
        "6. Wait for the service to be created.\n",
        "7. View your newly created Form Recognizer service in the Azure portal and on the **Quick Start** page, copy the **Key1** and **Endpoint** values and paste them in the code cell below, replacing **YOUR_FORM_KEY** and **YOUR_FORM_ENDPOINT**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "form_key = 'YOUR_FORM_KEY'\n",
        "form_endpoint = 'YOUR_FORM_ENDPOINT'\n",
        "\n",
        "print('Ready to use form recognizer at {} using key {}'.format(form_endpoint, form_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Now you're ready to use Form Recognizer to analyze a receipt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from python_code import form_recognizer\n",
        "import os\n",
        "\n",
        "try:\n",
        "    # Read the image data\n",
        "    image_path = os.path.join('data', 'vision', 'receipt.jpg')\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    \n",
        "    # Send the image data to Form Recognizer\n",
        "    receipt_data = form_recognizer.get_form_data(form_key, form_endpoint, data)\n",
        "\n",
        "    # Print the results\n",
        "    print('\\n')\n",
        "    for field in receipt_data:\n",
        "        print(field, receipt_data[field])\n",
        "\n",
        "except Exception as ex:\n",
        "    print('Error:', ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Note that Form Recognizer is able to interpret the data in the form, correctly identifying the merchant address and phone number, and the transaction date and time, as well as the line items, subtotal, tax, and total amounts.\n",
        "\n",
        "> **Note**: If you're curious about the code used to submit the request to Form Recognizer and process the results, look at the **form_recognizer.py** file in the **python_code** folder.\n",
        "\n",
        "## More Information\n",
        "\n",
        "- For more information about using the Computer Vision service for OCR, see [the Computer Vision documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-recognizing-text)\n",
        "- For moer information about the Form Recognizer service, see [the Form Recognizer documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/form-recognizer/index)"
      ]
    }
  ]
}